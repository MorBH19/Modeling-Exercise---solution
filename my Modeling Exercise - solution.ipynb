{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bb7c4c-452a-408f-ac26-b184f1e10862",
   "metadata": {},
   "source": [
    "# Modeling exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbae80-7f28-4596-945e-a0784e384e4c",
   "metadata": {},
   "source": [
    "## General Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5650156-42a5-4fb7-b74a-dec08bafd6c1",
   "metadata": {},
   "source": [
    "* Submission date: 14.5.2023\n",
    "* Submission Method: Link to your solution notebook in [this sheet](https://docs.google.com/spreadsheets/d/1GNPESGIhJpPb7LwMAyjF5qpJfZQak_mLkE3i5Y7a_VA/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e136158-e12c-4d15-9c47-3cd726054a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../src')\n",
    "import numpy as np\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53824c3-ccb9-4f1b-9420-bb0b57509862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0716a6-a156-49a2-83d2-24d5ef36af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import make_circles_dataframe, make_moons_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513df470-6d24-4611-9fe8-61c25397763a",
   "metadata": {},
   "source": [
    "## Fitting and Overfiting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e502f-4b12-4bf1-a981-7135b36e0830",
   "metadata": {},
   "source": [
    "The goal of the following exercise is to:\n",
    "* Observe overfitting due to insuffient data\n",
    "* Observe Overfitting due to overly complex model\n",
    "* Identify the overfitting point by looking at Train vs Test error dynamic\n",
    "* Observe how noise levels effect the needed data samples and model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b1cdc-fcfb-4ac7-841c-a5a534d569cd",
   "metadata": {},
   "source": [
    "To do so, you'll code an experiment in the first part, and analyze the experiment result in the second part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f343cc-0b25-4302-ab24-fc290c07b6ee",
   "metadata": {},
   "source": [
    "### Building an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a17e0-fb6f-479c-98bb-e5a6fc81e1ad",
   "metadata": {},
   "source": [
    "Code:\n",
    "\n",
    "1. Create data of size N with noise level of magnitude NL from datasets DS_NAME. \n",
    "1. Split it to training and validation data (no need for test set), use 80%-20%. \n",
    "1. Use Logistic regression and Choose one complex model of your choice: [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), [SVM with RBF kernel](https://scikit-learn.org/stable/modules/svm.html) with different `gamma` values or [Random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) with differnt number of `min_samples_split`. \n",
    "1. Train on the train set for different hyper parameter values. compute:\n",
    "   1. Classification accuracy on the training set (TRE)\n",
    "   1. Classification accuracy on the validation set (TESTE)\n",
    "   1. The difference beteen the two above (E_DIFF)\n",
    "1. Save DS_NAME, N, NL, CLF_NAME, K, TRE, TESTE, E_DIFF and the regularization/hyper param (K, gamma or min_samples_split and regularization value for the linear regression classifier)\n",
    "\n",
    "Repeat for:\n",
    "* DS_NAME in Moons, Circles\n",
    "* N (number of samples) in [5, 10, 50, 100, 1000, 10000]\n",
    "* NL (noise level) in [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "* For the complex model: 10 Values of hyper parameter of the complex model you've chosen.\n",
    "* For the linear model: 5 values of ridge (l2) regularization - [0.001, 0.01, 0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e0146-9c7a-4cf1-a83e-da9a11bcc796",
   "metadata": {},
   "source": [
    "### Analysing the expermient results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39e9c4-0332-4564-a674-52b99332ecfd",
   "metadata": {},
   "source": [
    "1. For SVM only, For dataset of size 10k and for each dataset, What are the best model params? How stable is it? \n",
    "1. For SVM only, For dataset of size 10k and for each dataset, What is the most stable model and model params? How good is it in comparison to other models? Explain using bias and variance terminoligy.\n",
    "1. Does regularization help for linear models? consider different datasets sizes. \n",
    "1. For a given noise level of your chioce, How does the train, test and difference error changes with increasing data sizes? (answer for svm and LR seperatly)\n",
    "1. For a given noise level of your chioce, How does the train, test and difference error changes with increasing model complexity? (answer for svm and LR seperatly)\n",
    "1. Are the noise level effect the number of datapoints needed to reach optimal test results? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fa350-4106-419c-9020-51986a0a716d",
   "metadata": {},
   "source": [
    "Bonus:\n",
    "\n",
    "* For SVM: Select one dataset and with 0.2 noise level. Identify the optimal model params, and visualize the decision boundry learned. \n",
    "  * Hint: Use a grid. See classification models notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf7262-12b1-4417-acd2-eb8365b04b11",
   "metadata": {},
   "source": [
    "## Tips and Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca4522e-a8e9-4476-90ac-d1a2679085f4",
   "metadata": {},
   "source": [
    "For buliding the experiment:\n",
    "\n",
    "* Start with one dataframe holding all the data for both datastes with different noise level. Use the `make_<dataset_name>_dataframe()` functions below, and add two columns, dataset_name and noise_level, before appending the new dataset to the rest of the datasets. Use `df = pd.DataFrame()` to start with an empty dataframe and using a loop, add data to it using `df = df.append(<the needed df here>)`. Verify that you have 10k samples for each dataset type and noise level by a proper `.value_counts()`. You can modify the \n",
    "* When you'll need an N samples data with a specific noise level, use `query()` and `head(n)` to get the needed dataset. \n",
    "* Use sklearn `train_test_split()` method to split the data with `test_size` and `random_state` parameters set correctly to ensure you are always splitting the data the same why for a given fold `k`. Read [the docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) if needed. \n",
    "* You can also not create your own data splitter, and instead use `model_selection.cross_validate()` from sklearn. You'll need to ask for the train erros as well as the test errors, see [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html).\n",
    "* Use prints in proper location to ensure the progress of the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f130c6-62ba-4490-983d-1d63470dc615",
   "metadata": {},
   "source": [
    "**If you get stuck, and need refernce, scroll to the end of the notebook to see more hints!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ffe8f-8c0b-4dc3-9cc2-9c3dc02852d7",
   "metadata": {},
   "source": [
    "## Moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a6b4c61-454e-4887-963f-61d5ec6bc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7392ae8f-32a7-4c77-a5be-34043ecd09ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180312</td>\n",
       "      <td>0.929281</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.491095</td>\n",
       "      <td>0.891292</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.796309</td>\n",
       "      <td>0.407228</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.638896</td>\n",
       "      <td>-0.080477</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.120858</td>\n",
       "      <td>0.454599</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y label\n",
       "0  0.180312  0.929281     A\n",
       "1  0.491095  0.891292     A\n",
       "2  0.796309  0.407228     A\n",
       "3  1.638896 -0.080477     B\n",
       "4 -1.120858  0.454599     A"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moons_df = make_moons_dataframe(n_samples=1000, noise_level=0.1)\n",
    "moons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c4d95c-3cad-4596-a8b3-29d67aa9859c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b8bb234f8d4105a8aca0bc08984be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='noise_level', max=0.5, step=0.05), Output()), _dom_c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact\n",
    "def plot_noisy_moons(noise_level = widgets.FloatSlider(value=0, min=0, max=0.5, step=0.05)):\n",
    "    moons_df = make_moons_dataframe(n_samples=1000, noise_level=noise_level)\n",
    "    return px.scatter(moons_df, x='x', y='y', color = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3299f00-00b8-4703-8568-35de2913a487",
   "metadata": {},
   "source": [
    "## Circles Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14b1498d-60e1-429e-a122-f120f5ed5720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.899405</td>\n",
       "      <td>-0.437116</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.920232</td>\n",
       "      <td>-0.391374</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.938734</td>\n",
       "      <td>-0.344643</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.974527</td>\n",
       "      <td>-0.224271</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.525269</td>\n",
       "      <td>-0.603401</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y label\n",
       "0  0.899405 -0.437116     A\n",
       "1 -0.920232 -0.391374     A\n",
       "2 -0.938734 -0.344643     A\n",
       "3 -0.974527 -0.224271     A\n",
       "4  0.525269 -0.603401     B"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circles_df = make_circles_dataframe(n_samples=500, noise_level=0)\n",
    "circles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ef522a-e381-42e1-b321-31b3d2f3b0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece5fda5cc5f42f3b22cbf5a051c3c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='noise_level', max=0.5, step=0.05), Output()), _dom_c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact\n",
    "def plot_noisy_circles(noise_level = widgets.FloatSlider(value=0, min=0, max=0.5, step=0.05)):\n",
    "    df = make_circles_dataframe(1000, noise_level)\n",
    "    return px.scatter(df, x='x', y='y', color = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68124026-b4bc-4022-81ee-158175f9e732",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c6e0b-b513-4bb7-b39e-ae3baa5b3e68",
   "metadata": {},
   "source": [
    "### More hints!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad0ce4-d598-4e30-ab43-dd9a064d8dd8",
   "metadata": {},
   "source": [
    "If you'll build the datasets dataframe correctly, you'll have **one** dataframe that has dataset_name and noise_level colmuns, as well as the regular x,y,label colmns. To unsure you've appended everything correctly, groupby the proper colmuns and look at the size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b5d99fc-102f-4b04-8962-1e85e8909cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use proper groupby statement to ensure the datasets dataframe contains data as expected. You should see the following result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58ae49-7d87-462b-b681-61536ae4bca8",
   "metadata": {},
   "source": [
    "You experiment code should look something like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b6dca06-01af-437e-8eb1-d559cdb96741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets_type = ['circles', 'moons']\n",
    "k_folds = 10\n",
    "n_samples = [10, 50, 100, 1000, 10000]\n",
    "noise_levels = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "clf_types = ['log_reg', 'svm']\n",
    "hp_range = <'Your hyper parameters ranges here'>\n",
    "regularization_values = <'Your regularization values here'>\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6332275-c8cb-46cc-8c05-b6b42ce0f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4ebc5-1279-430d-b2cf-9f255403bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25c9fc-ea4a-42af-8966-a4492e51994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_type = ['circles', 'moons']\n",
    "n_samples = [5, 10, 50, 100, 1000, 10000]\n",
    "noise_levels = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "k_folds = [10]\n",
    "clf_types = ['log_reg', 'svm']\n",
    "regularization_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gamma_range = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "results = pd.DataFrame(columns=['DS_NAME', 'N', 'NL', 'CLF_NAME', 'Hyper', 'RegVal', 'TRE', 'TESTE', 'E_DIFF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f223a817-9be2-40b0-8c45-5f7c1bb12f99",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (248489794.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\missm\\AppData\\Local\\Temp\\ipykernel_2380\\248489794.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    ds = datasets.query(<'your query here'>).head(n)\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for ds_type in datasets_type:\n",
    "    print(f'Working on {ds_type}')\n",
    "    for nl in noise_levels:\n",
    "        for n in n_samples:\n",
    "            ds = datasets.query(<'your query here'>).head(n)\n",
    "            print(f'Starting {k_folds}-fold cross validation for {ds_type} datasets with {n} samples and noise level {nl}. Going to train {clf_types} classifiers.')\n",
    "            for k in range(k_folds):\n",
    "                X, Y = <'Your code here'>\n",
    "                x_train,x_test,y_train,y_test= <'Your code here'>\n",
    "                for clf_type in clf_types:\n",
    "                    if clf_type == 'log_reg':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            train_acc, test_acc = <'Your code here'>\n",
    "                            results.append(<'Your code here'>)\n",
    "                    if clf_type == 'svm':\n",
    "                        for gamma in hp_range:\n",
    "                            train_acc, test_acc = <'Your code here'>\n",
    "                            results.append(<'Your code here'>)\n",
    "                    elif clf_type == 'svm':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            hp_range = [0.01, 0.1, 1, 10, 100]\n",
    "                            # hp_range = [0.01]\n",
    "                            for gamma in hp_range:\n",
    "                                clf = SVC(kernel='rbf', C=regularization_value, gamma=gamma)\n",
    "                                # cross_val_score(clf, X_train, y_train, cv=10)\n",
    "                                clf.fit(X_train, y_train)\n",
    "                                \n",
    "                                y_pred = clf.predict(X_test)\n",
    "                                y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                                # cross_val_score(clf, X_train, y_train, cv=10).mean()\n",
    "                                \n",
    "                                train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                                test_acc = accuracy_score(y_test, y_pred)\n",
    "                                \n",
    "                                df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                'Gamma':gamma,\n",
    "                                'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "                                # print('DS_NAME', ds_type,'N', n,'NL',nl, \n",
    "                                # 'CLF_NAME',clf_type,'K',k,'RegVal',regularization_value, \n",
    "                                # 'Gamma', gamma,\n",
    "                                # 'TRE', train_acc, 'TESTE', test_acc, 'E_DIFF', train_acc-test_acc)\n",
    "                                results = results.append(df2)\n",
    "                    \n",
    "                    elif clf_type == 'rf':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            hp_range = [0.01, 0.1, 1, 10, 100]\n",
    "                            # for gamma in hp_range:\n",
    "                            clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                            y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                            train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                            test_acc = accuracy_score(y_test, y_pred)\n",
    "                            df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                            'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                            'Gamma':gamma,\n",
    "                            'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "                            results = results.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5971f76-6f67-4c42-9349-8cba558c42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.reset_index(inplace=True)\n",
    "results.loc[results.TRE.idxmax()]\n",
    "results.loc[results.TESTE.idxmax()]\n",
    "results['E_DIFF']=abs(results['E_DIFF'])\n",
    "results.loc[results.E_DIFF.idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769cd23-3820-4be9-8b85-98be2fb82d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.For SVM only, For dataset of size 10k and for each dataset, What are the best model params? How stable is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142944dd-d0a8-465d-a188-10f54347f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 set-up\n",
    "results = pd.DataFrame(columns=['DS_NAME', 'N', 'NL', 'CLF_NAME', 'K','RegVal', 'TRE', 'TESTE', 'E_DIFF'])\n",
    "datasets_type = ['circles', 'moons']\n",
    "#datasets_type = ['circles']\n",
    "k_folds = [10]\n",
    "#n_samples = [50, 100, 1000, 10000]\n",
    "n_samples = [10000]\n",
    "# noise_levels = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "clf_types = ['svm']\n",
    "#clf_types = ['log_reg', 'svm']\n",
    "# hp_range = {'log_reg':{}, 'svm':{}, 'rand_forest':{}}\n",
    "# regularization_values = [0.001]\n",
    "regularization_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "noise_levels = [0.2]\n",
    "\n",
    "# clf_types = ['log_reg']\n",
    "# hp_range = {'log_reg':{}, 'svm':{'gamma':[], }, 'rand_forest':{}}\n",
    "\n",
    "gamma_range = [0.01, 0.1, 1, 10, 100]\n",
    "# gamma_range = [100]\n",
    "\n",
    "# pre proceeing of data\n",
    "moons_df['type']='moons'\n",
    "circles_df['type']='circles'\n",
    "\n",
    "df=pd.concat([circles_df, moons_df])\n",
    "df['x^2']=df['x']**2\n",
    "df['y^2']=df['y']**2\n",
    "df['x*y']=df['x']*df['y']\n",
    "\n",
    "df_original = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4b6cb-212d-4f93-a140-e3080cb4810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 code\n",
    "for ds_type in datasets_type:\n",
    "    print(f'Working on {ds_type}')\n",
    "    for nl in noise_levels:\n",
    "        for n in n_samples:\n",
    "            print('n: ',n)\n",
    "            df = df_original[df_original[\"type\"] == ds_type].head(n)\n",
    "            # print(f'Starting {k_folds}-fold cross validation for {ds_type} datasets with {n} samples and noise level {nl}. Going to train {clf_types} classifiers.')\n",
    "            for k in k_folds:\n",
    "                X = df.drop(['label', 'type'], axis=1)\n",
    "                y = df['label']\n",
    "                X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                for clf_type in clf_types:\n",
    "                    if clf_type == 'log_reg':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            clf = LogisticRegression(random_state=0, C=regularization_value, solver='liblinear')\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                            y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                            train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                            test_acc = accuracy_score(y_test, y_pred)\n",
    "                            df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                                'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                                'Gamma':'none',\n",
    "                                                'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "                            results = results.append(df2)\n",
    "                            \n",
    "\n",
    "                    elif clf_type == 'svm':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            hp_range = [0.01, 0.1, 1, 10, 100]\n",
    "                            # hp_range = [0.01]\n",
    "                            for gamma in hp_range:\n",
    "                                clf = SVC(kernel='rbf', C=regularization_value, gamma=gamma)\n",
    "                                # cross_val_score(clf, X_train, y_train, cv=10)\n",
    "                                clf.fit(X_train, y_train)\n",
    "                                \n",
    "                                y_pred = clf.predict(X_test)\n",
    "                                y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                                # cross_val_score(clf, X_train, y_train, cv=10).mean()\n",
    "                                \n",
    "                                train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                                test_acc = accuracy_score(y_test, y_pred)\n",
    "                                \n",
    "                                df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                'Gamma':gamma,\n",
    "                                'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "                                # print('DS_NAME', ds_type,'N', n,'NL',nl, \n",
    "                                # 'CLF_NAME',clf_type,'K',k,'RegVal',regularization_value, \n",
    "                                # 'Gamma', gamma,\n",
    "                                # 'TRE', train_acc, 'TESTE', test_acc, 'E_DIFF', train_acc-test_acc)\n",
    "                                results = results.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d5e27-b389-40f9-8ffb-20910bb0f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722daf2-6bd4-48d4-9cc6-68d296029c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_1=results[['Gamma','RegVal', 'TESTE','E_DIFF','DS_NAME']]\n",
    "circles_best_param=grid_1.query('DS_NAME==\"circles\"').sort_values(\"TESTE\",ascending=False)\n",
    "\n",
    "print(\"circles_best_params are: \")\n",
    "print(circles_best_param.head(1),\"\\n\")\n",
    "\n",
    "moons_best_param=grid_1.query('DS_NAME==\"moons\"').sort_values(\"TESTE\",ascending=False)\n",
    "\n",
    "print(\"moons_best_params are: \")\n",
    "print(moons_best_param.head(1),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cbd23-856c-4c94-befd-1c6f56eff547",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.For SVM only, For dataset of size 10k and for each dataset, What is the most stable model and model params? How good is it in comparison to other models? Explain using bias and variance terminoligy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6fa39-86b7-4277-916b-8938ba74a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_2=results[['Gamma','RegVal', 'TESTE','E_DIFF','DS_NAME','N']]\n",
    "grid_2=grid_2[grid_2['N']==10000]\n",
    "grid_2\n",
    "circles_most_stable_model_param=grid_2.query('DS_NAME==\"circles\"').sort_values('E_DIFF',ascending=True)\n",
    "\n",
    "print(\"Stable model is on that the differnce of accuracy between test and train is minimal i.e lowest varianc.\\n\\n\",\"circles most stable model param are: \")\n",
    "print(circles_most_stable_model_param.head(1),\"\\n\")\n",
    "\n",
    "moons_most_stable_model_param=grid_2.query('DS_NAME==\"moons\"').sort_values('E_DIFF',ascending=True)\n",
    "\n",
    "print(\"moons_most stable model param are: \")\n",
    "print(moons_most_stable_model_param.head(1),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6cd62-da12-4446-8b5f-4472d104fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Does regularization help for linear models? consider different datasets sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ca6f9-45b1-49c9-8b33-92e8cbece603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 set-up\n",
    "results = pd.DataFrame(columns=['DS_NAME', 'N', 'NL', 'CLF_NAME', 'K','RegVal', 'TRE', 'TESTE', 'E_DIFF'])\n",
    "datasets_type = ['circles']\n",
    "#datasets_type = ['circles']\n",
    "k_folds = [10]\n",
    "n_samples = [50, 100, 1000, 10000]\n",
    "#n_samples = [10000]\n",
    "noise_levels = [0.2]\n",
    "# noise_levels = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "clf_types = ['log_reg']\n",
    "#clf_types = ['log_reg', 'svm']\n",
    "# hp_range = {'log_reg':{}, 'svm':{}, 'rand_forest':{}}\n",
    "# regularization_values = [0.001]\n",
    "regularization_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# clf_types = ['log_reg']\n",
    "# hp_range = {'log_reg':{}, 'svm':{'gamma':[], }, 'rand_forest':{}}\n",
    "\n",
    "#gamma_range = [0.01, 0.1, 1, 10, 100]\n",
    "gamma_range = [100]\n",
    "\n",
    "# pre proceeing of data\n",
    "moons_df['type']='moons'\n",
    "circles_df['type']='circles'\n",
    "\n",
    "df=pd.concat([circles_df, moons_df])\n",
    "df['x^2']=df['x']**2\n",
    "df['y^2']=df['y']**2\n",
    "df['x*y']=df['x']*df['y']\n",
    "\n",
    "df_original = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ad0fe-9f7b-4085-98c1-1de05607c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 code\n",
    "for ds_type in datasets_type:\n",
    "    print(f'Working on {ds_type}')\n",
    "    for nl in noise_levels:\n",
    "        for n in n_samples:\n",
    "            print('n: ',n)\n",
    "            df = df_original[df_original[\"type\"] == ds_type].head(n)\n",
    "            # print(f'Starting {k_folds}-fold cross validation for {ds_type} datasets with {n} samples and noise level {nl}. Going to train {clf_types} classifiers.')\n",
    "            for k in k_folds:\n",
    "                X = df.drop(['label', 'type'], axis=1)\n",
    "                y = df['label']\n",
    "                X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "                for clf_type in clf_types:\n",
    "                    if clf_type == 'log_reg':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            clf = LogisticRegression(random_state=0, C=regularization_value, solver='liblinear')\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                            y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                            train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                            test_acc = accuracy_score(y_test, y_pred)\n",
    "                            df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                                'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                                'Gamma':'none',\n",
    "                                                'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "                            results = results.append(df2)\n",
    "                            \n",
    "\n",
    "                    elif clf_type == 'svm':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            hp_range = [0.01, 0.1, 1, 10, 100]\n",
    "                            # hp_range = [0.01]\n",
    "                            for gamma in hp_range:\n",
    "                                clf = SVC(kernel='rbf', C=regularization_value, gamma=gamma)\n",
    "                                # cross_val_score(clf, X_train, y_train, cv=10)\n",
    "                                clf.fit(X_train, y_train)\n",
    "                                \n",
    "                                y_pred = clf.predict(X_test)\n",
    "                                y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                                # cross_val_score(clf, X_train, y_train, cv=10).mean()\n",
    "                                \n",
    "                                train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                                test_acc = accuracy_score(y_test, y_pred)\n",
    "                                \n",
    "                                df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                'Gamma':gamma,\n",
    "                                'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "                                # print('DS_NAME', ds_type,'N', n,'NL',nl, \n",
    "                                # 'CLF_NAME',clf_type,'K',k,'RegVal',regularization_value, \n",
    "                                # 'Gamma', gamma,\n",
    "                                # 'TRE', train_acc, 'TESTE', test_acc, 'E_DIFF', train_acc-test_acc)\n",
    "                                results1 = results.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14341663-5cab-4c42-a67c-12e529f48a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['LOG_RegVal']=np.log10(results['RegVal'])\n",
    "results\n",
    "px.scatter(results.reset_index(), x='RegVal',y='TESTE',error_y='E_DIFF', color_discrete_sequence=px.colors.qualitative.G10 ,log_x=True, color='N', title='Figure 3.1 - Logistic regression accuracy as factor of Regularization value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17bb8d-c836-4994-9bc9-d52033437753",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. For a given noise level of your chioce, How does the train, test and difference error changes with increasing data sizes? (answer for svm and LR seperatly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea89440-a0a4-4529-b296-3a5edce4840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 set-up\n",
    "results4 = pd.DataFrame(columns=['DS_NAME', 'N', 'NL', 'CLF_NAME', 'K','RegVal', 'TRE', 'TESTE', 'E_DIFF'])\n",
    "results_moons = pd.DataFrame(columns=['DS_NAME', 'N', 'NL', 'CLF_NAME', 'K','RegVal', 'TRE', 'TESTE', 'E_DIFF'])\n",
    "datasets_type = ['circles', 'moons']\n",
    "#datasets_type = ['circles']\n",
    "k_folds = [10]\n",
    "n_samples = [50, 100, 1000, 10000]\n",
    "noise_levels = [0.3]\n",
    "clf_types = [ 'svm','log_reg']\n",
    "# hp_range = {'log_reg':{}, 'svm':{}, 'rand_forest':{}}\n",
    "regularization_values = [0.1]\n",
    "gamma_range = [1]\n",
    "\n",
    "moons_df['type']='moons'\n",
    "circles_df['type']='circles'\n",
    "\n",
    "df=pd.concat([circles_df, moons_df])\n",
    "df['x^2']=df['x']**2\n",
    "df['y^2']=df['y']**2\n",
    "df['x*y']=df['x']*df['y']\n",
    "\n",
    "df_original = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf41d8-d035-44ee-b879-3809d7a07b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 code\n",
    "for ds_type in datasets_type:\n",
    "    print(f'Working on {ds_type}')\n",
    "    for nl in noise_levels:\n",
    "        for n in n_samples:\n",
    "            print('n: ',n)\n",
    "            df = df_original[df_original[\"type\"] == ds_type].tail(n)\n",
    "            #print(df)\n",
    "            # print(f'Starting {k_folds}-fold cross validation for {ds_type} datasets with {n} samples and noise level {nl}. Going to train {clf_types} classifiers.')\n",
    "            for k in k_folds:\n",
    "                X = df.drop(['label', 'type'], axis=1)\n",
    "                y = df['label']\n",
    "                X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                for clf_type in clf_types:\n",
    "                    if clf_type == 'log_reg':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            clf = LogisticRegression(random_state=0, C=regularization_value, solver='liblinear')\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                            y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                            train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                            test_acc = accuracy_score(y_test, y_pred)\n",
    "                            df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                                'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                                'Gamma':'none',\n",
    "                                                'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "                            results4 = results4.append(df2)\n",
    "                    elif clf_type == 'svm':\n",
    "                        for regularization_value in regularization_values:\n",
    "                            hp_range = [1]\n",
    "                            # hp_range = [0.01]\n",
    "                            for gamma in hp_range:\n",
    "                                clf = SVC(kernel='rbf', C=regularization_value, gamma=gamma)\n",
    "                                # cross_val_score(clf, X_train, y_train, cv=10)\n",
    "                                clf.fit(X_train, y_train)\n",
    "                                \n",
    "                                y_pred = clf.predict(X_test)\n",
    "                                y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                                # cross_val_score(clf, X_train, y_train, cv=10).mean()\n",
    "                                \n",
    "                                train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                                test_acc = accuracy_score(y_test, y_pred)\n",
    "                               # print(\"y_test : \",y_test, \"y_pred : \", y_pred)\n",
    "                                \n",
    "                                df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                'Gamma':gamma,\n",
    "                                'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "                                # print('DS_NAME', ds_type,'N', n,'NL',nl, \n",
    "                                # 'CLF_NAME',clf_type,'K',k,'RegVal',regularization_value, \n",
    "                                # 'Gamma', gamma,\n",
    "                                # 'TRE', train_acc, 'TESTE', test_acc, 'E_DIFF', train_acc-test_acc)\n",
    "                                results4 = results4.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5604c-5564-4dd4-99ef-0b9a6744f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_Q4a= results4[results4['CLF_NAME']=='svm']\n",
    "#df[(df['TotalMarks'] > 50) & (df['TotalMarks'] < 80) ]\n",
    "#results_Q4a['LOG_N']=np.log10(results_Q4a['N'].astype(float))\n",
    "\n",
    "px.scatter(results_Q4a.reset_index(), x='N',y='TESTE',error_y='E_DIFF', color='DS_NAME',log_x=True,\n",
    "           title='Figure 4.1 - Test accuracy and E_diff as function of (log) Number of samples, <br> Clasifier is SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894d465-956e-47ed-a7e7-c57154c3c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae5e3d-520d-4b6b-93de-49ff7099d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_Q4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022ef93-a0bb-44a1-8c26-5b6b706b225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_Q4a2= results4[results4['CLF_NAME']=='log_reg']\n",
    "\n",
    "px.scatter(results_Q4a2.reset_index(), x='N',y='TESTE',error_y='E_DIFF', color='DS_NAME',log_x=True,\n",
    "           title='Figure 4.1 - Test accuracy and E_diff as function of (log) Number of samples, <br> Clasifier is Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa745e47-190b-4b42-a03d-180adac725ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_Q4a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7545d4-f167-4cb0-b24d-c5826885e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(results_Q4a.reset_index(), x='N',y='TRE', color='DS_NAME',log_x=True,\n",
    "           title=\"Figure 4.2 - Train accuracy and E_diff as function of log Number of samples<br> Clasifier as parameter, Dataset ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d8a1f-308e-4715-be09-9e36bf3d55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_Q4b= results4[results4['CLF_NAME']=='log_reg']\n",
    "px.scatter(results_Q4b.reset_index(), x='N',y='TESTE',error_y='E_DIFF', color='DS_NAME',log_x=True,\n",
    "           title='Figure 4.1(b) - Test accuracy and E_diff as function of log Number of samples, <br> Clasifier as parameter (Circles)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee40ac8-c654-43d5-b796-bf382bc87ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. For a given noise level of your chioce, How does the train, test and difference error changes with increasing model complexity? (answer for svm and LR seperatly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e566ce-4097-486e-bc7a-bcc716e53396",
   "metadata": {},
   "outputs": [],
   "source": [
    "results5 = pd.DataFrame(columns=['DS_NAME', 'N', 'NL', 'CLF_NAME', 'K','RegVal', 'TRE', 'TESTE', 'E_DIFF'])\n",
    "datasets_type = ['circles', 'moons']\n",
    "k_folds = [10]\n",
    "n_samples = [10000]\n",
    "noise_levels = [0.2]\n",
    "\n",
    "clf_types = ['log_reg', 'svm']\n",
    "regularization_values = [1]\n",
    "\n",
    "moons_df['type']='moons'\n",
    "circles_df['type']='circles'\n",
    "\n",
    "df=pd.concat([circles_df, moons_df])\n",
    "df['x^2']=df['x']**2\n",
    "df['y^2']=df['y']**2\n",
    "df['x*y']=df['x']*df['y']\n",
    "df_original = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70e6d1-20f5-4ed1-bf73-050d4321cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_particial = ['label','type']\n",
    "for feature in feature_list_full:\n",
    "    feature_list_particial.append(feature)\n",
    "    num_features= len(feature_list_particial)-2\n",
    "    for ds_type in datasets_type:\n",
    "        print(f'Working on {ds_type}')\n",
    "        for nl in noise_levels:\n",
    "            for n in n_samples:\n",
    "                print('n: ',n)\n",
    "                df = df_original[df_original[\"type\"] == ds_type].head(n)\n",
    "                df = df[feature_list_particial]\n",
    "                # print(f'Starting {k_folds}-fold cross validation for {ds_type} datasets with {n} samples and noise level {nl}. Going to train {clf_types} classifiers.')\n",
    "                for k in k_folds:\n",
    "                    X = df.drop(['label', 'type'], axis=1)\n",
    "                    y = df['label']\n",
    "                    X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "                    for clf_type in clf_types:\n",
    "                        if clf_type == 'log_reg':\n",
    "                            for regularization_value in regularization_values:\n",
    "                                clf = LogisticRegression(random_state=0, C=regularization_value, solver='liblinear')\n",
    "                                clf.fit(X_train, y_train)\n",
    "                                y_pred = clf.predict(X_test)\n",
    "                                y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "                                train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                                test_acc = accuracy_score(y_test, y_pred)\n",
    "                                df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                                    'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                                    'Gamma':'none',\n",
    "                                                    'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc, \n",
    "                                                    'FEATURES' : str(feature_list_particial),\n",
    "                                                   'NUMBER_FEATURES':num_features}, index=[0])\n",
    "                                results5 = results5.append(df2)\n",
    "\n",
    "\n",
    "                        elif clf_type == 'svm':\n",
    "                            for regularization_value in regularization_values:\n",
    "                                # hp_range = [0.01, 0.1, 1, 10, 100]\n",
    "                                hp_range = [1]\n",
    "                                for gamma in hp_range:\n",
    "                                    clf = SVC(kernel='rbf', C=regularization_value, gamma=gamma)\n",
    "                                    clf.fit(X_train, y_train)\n",
    "\n",
    "                                    y_pred = clf.predict(X_test)\n",
    "                                    y_train_pred = cross_val_predict(clf, X_train, y_train, cv=k)\n",
    "\n",
    "                                    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "                                    test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                                    df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "                                    'CLF_NAME':clf_type,'K':k,'RegVal':regularization_value, \n",
    "                                    'Gamma':gamma,\n",
    "                                    'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc,'FEATURES' : str(feature_list_particial),'NUMBER_FEATURES':num_features}, index=[0])\n",
    "                                    results5 = results5.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0071b6-25d9-4527-9ac3-43707f9acc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c8ac2-e7a5-4ecd-a9bb-c37eb0bda7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results5_moons = results5[results5['DS_NAME']=='moons']\n",
    "px.scatter(results5_moons.reset_index(), x='NUMBER_FEATURES',y='TESTE', color='CLF_NAME',log_x=False,\n",
    "           title=\"Figure 5 - Test accuracy as function of Number of features<br> Noise Level as parameter, Dataset = Moons, Regularizat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a761d25-834c-41a3-a3d8-dcfa03095a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results5_circle = results5[results5['DS_NAME']=='circles']\n",
    "px.scatter(results5_circle.reset_index(), x='NUMBER_FEATURES',y='TESTE', color='CLF_NAME',log_x=False,\n",
    "           title=\"Figure 5 - Test accuracy as function of Number of features<br> Noise Level as parameter, Dataset = circles, Regularization value = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d09ff7-2aaf-44bb-8a03-a9ba38da387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Are the noise level effect the number of datapoints needed to reach optimal test results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba40e0e-3ee6-4f6c-9efd-35c27dd719df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6\n",
    "results = pd.DataFrame(columns=['DS_NAME', 'N', 'NL', 'CLF_NAME', 'K','RegVal', 'TRE', 'TESTE', 'E_DIFF'])\n",
    "# datasets_type = ['circles', 'moons']\n",
    "datasets_type = ['moons']\n",
    "k_folds = [10]\n",
    "n_samples = [50, 100, 1000, 5000,10000]\n",
    "# n_samples = [10000]\n",
    "\n",
    "noise_levels = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "clf_types = ['svm']\n",
    "# hp_range = {'log_reg':{penalty:['l1','l2','elasticnet'],\n",
    "#                        solver:['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'} , 'svm':{}}\n",
    "regularization_values = [0.1]\n",
    "\n",
    "\n",
    "# gamma_range = [0.01, 0.1, 1, 10, 100]\n",
    "gamma_range = [1]\n",
    "\n",
    "\n",
    "# pre proceeing of data\n",
    "moons_df['type']='moons'\n",
    "circles_df['type']='circles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab66d2-9cee-40a4-9217-4f2740cb1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 imports classifiers\n",
    "#6-Are the noise level effect the number of datapoints needed to reach optimal test results?\n",
    "for nl in noise_levels:\n",
    "\n",
    "    for n in n_samples:\n",
    "        df = make_moons_dataframe(n, nl)\n",
    "        X = df.drop(['label'], axis=1)\n",
    "        y = df['label']\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        # print(f'Starting {k_folds}-fold cross validation for {ds_type} datasets with {n} samples and noise level {nl}. Going to train {clf_types} classifiers.')\n",
    "        clf = SVC(kernel='rbf', C=0.1, gamma=1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_train_pred = cross_val_predict(clf, X_train, y_train, cv=10)\n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        df2 = pd.DataFrame({'DS_NAME':ds_type,'N':n,'NL':nl, \n",
    "        'CLF_NAME':'svm','K':10,'RegVal':regularization_value, \n",
    "        'Gamma':gamma,\n",
    "        'TRE':train_acc, 'TESTE': test_acc, 'E_DIFF': train_acc-test_acc}, index=[0])\n",
    "        results = results.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fa541-97bb-490e-9541-830f77e6979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149596c-e05c-4737-a586-1681342e388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(results.reset_index(), x='N',y='TESTE', color='NL',log_x=True,\n",
    "           title=\"Figure 6 - Test accuracy as function of log Number of samples<br> Noise Level as parameter, Dataset = Moons, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b8d933f-0b71-43ca-ad3b-eeb8e28ac063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A deef looking in the range of n= 1000, 5000 , 10000 shows that when it is more moisy, the train needs more sample to rrach optimal reslults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
